{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f074256",
   "metadata": {},
   "source": [
    "# Reward Model Testing\n",
    "\n",
    "This notebook is used to test the trained reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3893b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd42f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model path\n",
    "MODEL_PATH = \"Qwen2.5-1.5B-Instruct-ultrafeedback_binarized-reward-num_labels_1_wo_filter\"\n",
    "\n",
    "# Global variables to store the model\n",
    "tokenizer = None\n",
    "model = None\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592e815",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Load reward model\"\"\"\n",
    "    global tokenizer, model, device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading model from {MODEL_PATH}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        num_labels=1,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Load model\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b37cc",
   "metadata": {},
   "source": [
    "## 2. Define Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c18f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward_score(prompt, response):\n",
    "    \"\"\"\n",
    "    Input prompt and response, return reward score\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input question\n",
    "        response: Model response\n",
    "    \n",
    "    Returns:\n",
    "        float: reward score\n",
    "    \"\"\"\n",
    "    # Use chat template to format input\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=False, max_length=None).to(device)\n",
    "    \n",
    "    # Get reward score\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        reward = outputs.logits[0, 0].item()\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdeadb5",
   "metadata": {},
   "source": [
    "## 3. Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed49dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"prompt\": \"How do I learn programming?\",\n",
    "        \"good_response\": \"To learn programming, start with the basics: 1. Choose a beginner-friendly language like Python. 2. Systematically learn syntax and fundamentals. 3. Practice with exercises and projects. 4. Read high-quality code. 5. Join communities to learn and share.\",\n",
    "        \"bad_response\": \"I don't know, Google it yourself.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is Artificial Intelligence?\",\n",
    "        \"good_response\": \"Artificial Intelligence (AI) is a branch of computer science focused on building systems that simulate human intelligence. It encompasses machine learning, deep learning, and natural language processing, enabling applications in image recognition, speech processing, autonomous driving, and more.\",\n",
    "        \"bad_response\": \"It's just machines.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Recommend a good book.\",\n",
    "        \"good_response\": \"I recommend 'The Three-Body Problem', a sci-fi trilogy by Liu Cixin. It explores the contact between humanity and extraterrestrial civilization with profound philosophy and vivid imagination. A Hugo Award winner, it's a masterpiece of Chinese science fiction.\",\n",
    "        \"bad_response\": \"Read whatever you find.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa14f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Start Testing Reward Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    prompt = case[\"prompt\"]\n",
    "    good_response = case[\"good_response\"]\n",
    "    bad_response = case[\"bad_response\"]\n",
    "    \n",
    "    good_score = get_reward_score(prompt, good_response)\n",
    "    bad_score = get_reward_score(prompt, bad_response)\n",
    "    \n",
    "    print(f\"\\n[Test Case {i}]\")\n",
    "    print(f\"Question: {prompt}\")\n",
    "    print(f\"\\nGood Response: {good_response}\")\n",
    "    print(f\"Score: {good_score:.4f}\")\n",
    "    print(f\"\\nBad Response: {bad_response}\")\n",
    "    print(f\"Score: {bad_score:.4f}\")\n",
    "    print(f\"\\nDiff: {good_score - bad_score:.4f}\")\n",
    "    print(f\"Order Correct: {'✓' if good_score > bad_score else '✗'}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01503827",
   "metadata": {},
   "source": [
    "## 4. Custom Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single example\n",
    "custom_prompt = \"How to read a file in Python?\"\n",
    "custom_response = \"In Python, you can read a file using the open() function. For example: with open('file.txt', 'r') as f: content = f.read()\"\n",
    "\n",
    "score = get_reward_score(custom_prompt, custom_response)\n",
    "print(f\"Question: {custom_prompt}\")\n",
    "print(f\"Response: {custom_response}\")\n",
    "print(f\"Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test your own examples here\n",
    "my_prompt = \"Your question\"\n",
    "my_response = \"Your response\"\n",
    "\n",
    "my_score = get_reward_score(my_prompt, my_response)\n",
    "print(f\"Score: {my_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca00f86",
   "metadata": {},
   "source": [
    "## 5. Test Accuracy on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a891576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading ultrafeedback_binarized training set...\")\n",
    "dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "print(f\"\\nStart testing accuracy on all samples...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for i, example in enumerate(dataset):\n",
    "    prompt = example['chosen'][0][\"content\"]\n",
    "    chosen = example[\"chosen\"][-1][\"content\"]  # Get the last assistant message\n",
    "    rejected = example[\"rejected\"][-1][\"content\"]\n",
    "    \n",
    "    # Calculate scores\n",
    "    chosen_score = get_reward_score(prompt, chosen)\n",
    "    rejected_score = get_reward_score(prompt, rejected)\n",
    "    \n",
    "    # Check correctness\n",
    "    if chosen_score > rejected_score:\n",
    "        correct_count += 1\n",
    "        result = \"✓\"\n",
    "    else:\n",
    "        result = \"✗\"\n",
    "    \n",
    "    total_count += 1\n",
    "    \n",
    "    # Print progress every 1000 samples\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        current_acc = correct_count / total_count * 100\n",
    "        print(f\"Progress: {i+1}/{len(dataset)}, Current Accuracy: {current_acc:.2f}%\")\n",
    "\n",
    "# Calculate final accuracy\n",
    "accuracy = correct_count / total_count * 100\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFinal Result:\")\n",
    "print(f\"Correct Count: {correct_count}/{total_count}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36642d7b",
   "metadata": {},
   "source": [
    "## 6. Analyze Dataset Context Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Collect token lengths for all samples\n",
    "print(\"Analyzing dataset context length distribution...\")\n",
    "print(f\"Total samples in dataset: {len(dataset)}\")\n",
    "\n",
    "# Store lengths for different parts\n",
    "prompt_lengths = []\n",
    "chosen_response_lengths = []\n",
    "rejected_response_lengths = []\n",
    "full_chosen_lengths = []\n",
    "full_rejected_lengths = []\n",
    "\n",
    "# Analyze entire dataset\n",
    "for i, example in enumerate(dataset):\n",
    "    prompt = example['chosen'][0][\"content\"]\n",
    "    chosen = example[\"chosen\"][-1][\"content\"]\n",
    "    rejected = example[\"rejected\"][-1][\"content\"]\n",
    "    \n",
    "    # Count prompt length\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=False)\n",
    "    prompt_lengths.append(prompt_inputs['input_ids'].shape[1])\n",
    "    \n",
    "    # Count chosen response length\n",
    "    chosen_inputs = tokenizer(chosen, return_tensors=\"pt\", truncation=False)\n",
    "    chosen_response_lengths.append(chosen_inputs['input_ids'].shape[1])\n",
    "    \n",
    "    # Count rejected response length\n",
    "    rejected_inputs = tokenizer(rejected, return_tensors=\"pt\", truncation=False)\n",
    "    rejected_response_lengths.append(rejected_inputs['input_ids'].shape[1])\n",
    "    \n",
    "    # Count full conversation length (prompt + response)\n",
    "    messages_chosen = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": chosen}\n",
    "    ]\n",
    "    text_chosen = tokenizer.apply_chat_template(messages_chosen, tokenize=False, add_generation_prompt=False)\n",
    "    inputs_chosen = tokenizer(text_chosen, return_tensors=\"pt\", truncation=False)\n",
    "    full_chosen_lengths.append(inputs_chosen['input_ids'].shape[1])\n",
    "    \n",
    "    messages_rejected = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": rejected}\n",
    "    ]\n",
    "    text_rejected = tokenizer.apply_chat_template(messages_rejected, tokenize=False, add_generation_prompt=False)\n",
    "    inputs_rejected = tokenizer(text_rejected, return_tensors=\"pt\", truncation=False)\n",
    "    full_rejected_lengths.append(inputs_rejected['input_ids'].shape[1])\n",
    "    \n",
    "    # Print progress every 1000 samples\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"Processed: {i+1}/{len(dataset)} samples\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "prompt_lengths = np.array(prompt_lengths)\n",
    "chosen_response_lengths = np.array(chosen_response_lengths)\n",
    "rejected_response_lengths = np.array(rejected_response_lengths)\n",
    "full_chosen_lengths = np.array(full_chosen_lengths)\n",
    "full_rejected_lengths = np.array(full_rejected_lengths)\n",
    "all_full_lengths = np.concatenate([full_chosen_lengths, full_rejected_lengths])\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Context Length Statistics:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[Prompt Length Statistics]\")\n",
    "print(f\"  Max Length: {np.max(prompt_lengths)}\")\n",
    "print(f\"  Min Length: {np.min(prompt_lengths)}\")\n",
    "print(f\"  Mean Length: {np.mean(prompt_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(prompt_lengths):.2f}\")\n",
    "print(f\"  95th Percentile: {np.percentile(prompt_lengths, 95):.2f}\")\n",
    "\n",
    "print(\"\\n[Chosen Response Length Statistics]\")\n",
    "print(f\"  Max Length: {np.max(chosen_response_lengths)}\")\n",
    "print(f\"  Min Length: {np.min(chosen_response_lengths)}\")\n",
    "print(f\"  Mean Length: {np.mean(chosen_response_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(chosen_response_lengths):.2f}\")\n",
    "print(f\"  95th Percentile: {np.percentile(chosen_response_lengths, 95):.2f}\")\n",
    "\n",
    "print(\"\\n[Rejected Response Length Statistics]\")\n",
    "print(f\"  Max Length: {np.max(rejected_response_lengths)}\")\n",
    "print(f\"  Min Length: {np.min(rejected_response_lengths)}\")\n",
    "print(f\"  Mean Length: {np.mean(rejected_response_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(rejected_response_lengths):.2f}\")\n",
    "print(f\"  95th Percentile: {np.percentile(rejected_response_lengths, 95):.2f}\")\n",
    "\n",
    "print(\"\\n[Full Conversation Length Statistics (Prompt + Response)]\")\n",
    "print(f\"  Total Conversations: {len(all_full_lengths)}\")\n",
    "print(f\"  Max Length: {np.max(all_full_lengths)}\")\n",
    "print(f\"  Min Length: {np.min(all_full_lengths)}\")\n",
    "print(f\"  Mean Length: {np.mean(all_full_lengths):.2f}\")\n",
    "print(f\"  Median: {np.median(all_full_lengths):.2f}\")\n",
    "print(f\"  95th Percentile: {np.percentile(all_full_lengths, 95):.2f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Prompt Length Distribution\n",
    "axes[0, 0].hist(prompt_lengths, bins=80, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0, 0].axvline(np.mean(prompt_lengths), color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Mean: {np.mean(prompt_lengths):.2f}')\n",
    "axes[0, 0].axvline(np.median(prompt_lengths), color='green', linestyle='--', linewidth=2, \n",
    "                    label=f'Median: {np.median(prompt_lengths):.2f}')\n",
    "axes[0, 0].set_xlabel('Token Length', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Prompt Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Response Length Distribution Comparison\n",
    "axes[0, 1].hist(chosen_response_lengths, bins=80, alpha=0.6, color='green', label='Chosen', edgecolor='black')\n",
    "axes[0, 1].hist(rejected_response_lengths, bins=80, alpha=0.6, color='red', label='Rejected', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Token Length', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('Response Length Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Full Conversation Length Distribution\n",
    "axes[1, 0].hist(all_full_lengths, bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1, 0].axvline(np.mean(all_full_lengths), color='red', linestyle='--', linewidth=2, \n",
    "                    label=f'Mean: {np.mean(all_full_lengths):.2f}')\n",
    "axes[1, 0].axvline(np.median(all_full_lengths), color='green', linestyle='--', linewidth=2, \n",
    "                    label=f'Median: {np.median(all_full_lengths):.2f}')\n",
    "axes[1, 0].axvline(512, color='orange', linestyle='--', linewidth=2, label='512 tokens')\n",
    "axes[1, 0].set_xlabel('Token Length', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Full Conversation Length Distribution (Prompt + Response)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Boxplot Comparison\n",
    "box_data = [prompt_lengths, chosen_response_lengths, rejected_response_lengths]\n",
    "bp = axes[1, 1].boxplot(box_data, labels=['Prompt', 'Chosen\\nResponse', 'Rejected\\nResponse'], \n",
    "                         patch_artist=True, showmeans=True)\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "axes[1, 1].set_ylabel('Token Length', fontsize=12)\n",
    "axes[1, 1].set_title('Length Comparison Boxplot', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show percentage of samples in different length ranges\n",
    "print(\"\\n[Full Conversation Length Distribution]\")\n",
    "print(\"-\" * 80)\n",
    "ranges = [(0, 128), (128, 256), (256, 512), (512, 1024), (1024, 2048), (2048, float('inf'))]\n",
    "for start, end in ranges:\n",
    "    if end == float('inf'):\n",
    "        count = np.sum(all_full_lengths > start)\n",
    "        print(f\"> {start} tokens: {count} ({count/len(all_full_lengths)*100:.2f}%)\")\n",
    "    else:\n",
    "        count = np.sum((all_full_lengths > start) & (all_full_lengths <= end))\n",
    "        print(f\"{start}-{end} tokens: {count} ({count/len(all_full_lengths)*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
